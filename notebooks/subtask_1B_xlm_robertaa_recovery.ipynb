{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakib911Hossan/hate_speech_detection/blob/main/subtask_1B_xlm_robertaa_recovery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noik9q9c7Bhm"
      },
      "source": [
        "# [Hate Speech Identification Shared Task](https://multihate.github.io/): Subtask 1B at [BLP Workshop](https://blp-workshop.github.io/) @IJCNLP-AACL 2025\n",
        "\n",
        "This shared task is designed to identify the type of hate, its severity, and the targeted group from social media content. The goal is to develop robust systems that advance research in this area.\n",
        "\n",
        "In this subtask, given a Bangla text collected from YouTube comments, categorize whether the hate towards individuals, organizations, communities, or society."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSxBhCps7oBf"
      },
      "source": [
        "### Downloading dataset from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvwQNYHk6kV5"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_train.tsv\n",
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1B/blp25_hatespeech_subtask_1B_dev_test.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYZ96DWt-TZk"
      },
      "source": [
        "### installing required libraries.\n",
        " - transformers\n",
        " - datasets\n",
        " - evaluate\n",
        " - accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLJh5GGU-xET"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "# !pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXhVWUJ3A_hx"
      },
      "source": [
        "#### importing required libraries and setting up logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIUAU0rRBOmR"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP6CdL7NHpxJ"
      },
      "source": [
        "### Defining the training, validation, and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMzfE34iHyGV"
      },
      "outputs": [],
      "source": [
        "train_file = 'blp25_hatespeech_subtask_1B_train.tsv'\n",
        "validation_file = 'blp25_hatespeech_subtask_1B_dev.tsv'\n",
        "test_file = 'blp25_hatespeech_subtask_1B_dev_test.tsv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w59H3fOnLUcG"
      },
      "source": [
        "### Disable wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRQSQF6MLYrB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_w4YehCgX4"
      },
      "source": [
        "### Setting up the training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-GUUNj0BPbu"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "# ðŸŽ¯ OPTIMIZED CONFIGURATION BASED ON YOUR RESULTS\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./xlm_roberta_recovery/\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    # ðŸ”§ ADJUSTED LEARNING SCHEDULE - Your model peaked early then overfitted\n",
        "    learning_rate=10e-6,                    # Lower LR for more stable convergence\n",
        "    num_train_epochs=8,                    # More epochs with early stopping\n",
        "    warmup_ratio=0.15,                     # Longer warmup for stability\n",
        "    lr_scheduler_type=\"cosine\",            # Smoother LR decay\n",
        "    # ðŸ›¡ï¸ STRONGER REGULARIZATION - Combat the overfitting you observed\n",
        "    weight_decay=0.01,                     # Increase from your 0.01\n",
        "    max_grad_norm=0.5,                     # Tighter gradient clipping\n",
        "    dataloader_drop_last=True,             # More consistent batch sizes\n",
        "\n",
        "    # âœ… KEEP YOUR SUCCESSFUL BATCH CONFIGURATION\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # ðŸŽ¯ EARLY STOPPING - Prevent the Epoch 4 overfitting you experienced\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # ðŸ“Š COMPREHENSIVE MONITORING\n",
        "    logging_steps=250,                     # More frequent logging\n",
        "    eval_steps=None,                       # Eval every epoch\n",
        "    save_total_limit=3,                    # Keep only best 3 checkpoints\n",
        "\n",
        "    # ðŸ”§ SYSTEM OPTIMIZATIONS\n",
        "    report_to=None,\n",
        "    dataloader_num_workers=2,\n",
        "    fp16=True,                            # Mixed precision for efficiency\n",
        "    group_by_length=True,                 # Batch similar lengths together\n",
        ")\n",
        "\n",
        "# ðŸ›‘ MANDATORY EARLY STOPPING - Based on your overfitting pattern\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,            # Stop after 2 epochs without improvement\n",
        "    early_stopping_threshold=0.001        # Minimum improvement threshold\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ðŸŽ¯ OPTIMIZED DATA PARAMETERS\n",
        "max_train_samples = None\n",
        "max_eval_samples = None\n",
        "max_predict_samples = None\n",
        "max_seq_length = 512\n",
        "batch_size = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q4deAnUJ0iI"
      },
      "outputs": [],
      "source": [
        "transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "log_level = training_args.get_process_log_level()\n",
        "logger.setLevel(log_level)\n",
        "datasets.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.set_verbosity(log_level)\n",
        "transformers.utils.logging.enable_default_handler()\n",
        "transformers.utils.logging.enable_explicit_format()\n",
        "logger.warning(\n",
        "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        ")\n",
        "logger.info(f\"Training/evaluation parameters {training_args}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgkvwlbFHVo5"
      },
      "source": [
        "#### Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-De1tz5qHYre"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Change only the model name\n",
        "# Model setup\n",
        "model_name = 'xlm-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=5  # Your 5 classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqrrDbcKN8n"
      },
      "source": [
        "#### setting the random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvKpoxaQKTB6"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgNrs7AhKdvl"
      },
      "source": [
        "#### Loading data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDwaW8AnKcgD"
      },
      "outputs": [],
      "source": [
        "l2id = {'None': 0, 'Society': 1, 'Organization': 2, 'Community': 3, 'Individual': 4}\n",
        "train_df = pd.read_csv(train_file, sep='\\t')\n",
        "# print(train_df['label'])\n",
        "train_df['label'] = train_df['label'].map(l2id).fillna(0).astype(int)\n",
        "train_df = Dataset.from_pandas(train_df)\n",
        "validation_df = pd.read_csv(validation_file, sep='\\t')\n",
        "validation_df['label'] = validation_df['label'].map(l2id).fillna(0).astype(int)\n",
        "validation_df = Dataset.from_pandas(validation_df)\n",
        "test_df = pd.read_csv(test_file, sep='\\t')\n",
        "#test_df['label'] = test_df['label'].map(l2id)\n",
        "test_df = Dataset.from_pandas(test_df)\n",
        "\n",
        "data_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
        "for key in data_files.keys():\n",
        "    logger.info(f\"loading a local file for {key}\")\n",
        "raw_datasets = DatasetDict(\n",
        "    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1swECfaTuJl"
      },
      "outputs": [],
      "source": [
        "len(test_df['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbnxaKCbq2c7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJhNu7tPQ2RU"
      },
      "source": [
        "##### Extracting number of unique labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTl6NNPmOXhO"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "print(label_list)\n",
        "label_list.sort()  # sort the labels for determine\n",
        "num_labels = len(label_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1dpoOAPRJnN"
      },
      "source": [
        "### Loading Pretrained Configuration, Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmAaMuBuRQd2"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    finetuning_task=None,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        "    use_fast=True,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    from_tf=bool(\".ckpt\" in model_name),\n",
        "    config=config,\n",
        "    cache_dir=None,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=None,\n",
        "    ignore_mismatched_sizes=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7PIQVypeTf4"
      },
      "source": [
        "#### Preprocessing the raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqO3YWAZelhd"
      },
      "outputs": [],
      "source": [
        "non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "sentence1_key= non_label_column_names[1]\n",
        "\n",
        "# Padding strategy\n",
        "padding = \"max_length\"\n",
        "\n",
        "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
        "label_to_id = None\n",
        "if (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n",
        "    # Some have all caps in their config, some don't.\n",
        "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
        "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
        "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
        "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
        "            \"\\nIgnoring the model labels as a result.\",)\n",
        "\n",
        "if label_to_id is not None:\n",
        "    model.config.label2id = label_to_id\n",
        "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "\n",
        "if 128 > tokenizer.model_max_length:\n",
        "    logger.warning(\n",
        "        f\"The max_seq_length passed ({128}) is larger than the maximum length for the\"\n",
        "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\n",
        "max_seq_length = min(128, tokenizer.model_max_length)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the texts\n",
        "    args = (\n",
        "        (examples[sentence1_key],))\n",
        "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "    # Map labels to IDs (not necessary for GLUE tasks)\n",
        "    if label_to_id is not None and \"label\" in examples:\n",
        "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "    return result\n",
        "raw_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    load_from_cache_file=True,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASxWKiqifb_g"
      },
      "source": [
        "#### Finalize the training data for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHoDqrBGgD6F"
      },
      "outputs": [],
      "source": [
        "if \"train\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a train dataset\")\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "if max_train_samples is not None:\n",
        "    max_train_samples_n = min(len(train_dataset), max_train_samples)\n",
        "    train_dataset = train_dataset.select(range(max_train_samples_n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqME25nm-hwo"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k72vUTSigOzZ"
      },
      "source": [
        "#### Finalize the development/evaluation data for evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqrW8ospgUYZ"
      },
      "outputs": [],
      "source": [
        "if \"validation\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a validation dataset\")\n",
        "eval_dataset = raw_datasets[\"validation\"]\n",
        "if max_eval_samples is not None:\n",
        "    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n",
        "    eval_dataset = eval_dataset.select(range(max_eval_samples_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7sVqp3hgU4i"
      },
      "source": [
        "#### Finalize the test data for predicting the unseen test data using the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0dBjIQggcYs"
      },
      "outputs": [],
      "source": [
        "if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
        "    raise ValueError(\"requires a test dataset\")\n",
        "predict_dataset = raw_datasets[\"test\"]\n",
        "if max_predict_samples is not None:\n",
        "    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n",
        "    predict_dataset = predict_dataset.select(range(max_predict_samples_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqbo1xzRge36"
      },
      "source": [
        "#### Log a few random samples from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIO2bxSVgkLb"
      },
      "outputs": [],
      "source": [
        "for index in random.sample(range(len(train_dataset)), 3):\n",
        "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAcn0Pc8gogF"
      },
      "source": [
        "#### Get the metric function `accuracy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMWMQdaUgvAq"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foWUyuBHgxbA"
      },
      "source": [
        "#### Predictions and label_ids field and has to return a dictionary string to float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3VqxkqcgxCC"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNWK1Hfbg8-o"
      },
      "source": [
        "#### Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w6lNh-OhJLC"
      },
      "outputs": [],
      "source": [
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nYlugPRhNbg"
      },
      "source": [
        "#### Initialize our Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-rwWO7wOpok"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.remove_columns(\"id\")\n",
        "eval_dataset = eval_dataset.remove_columns(\"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeJco0JOhPHx"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUxWn9HrhqRM"
      },
      "source": [
        "#### Training our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B681qnPFhtY0"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaaRglkwllSp"
      },
      "source": [
        "#### Saving the tokenizer too for easy upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UwoMEbAloMx"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I7HqSVbRZMs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9zCKBGEhwb7"
      },
      "source": [
        "#### Evaluating our model on validation/development data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YClw3dXTh17u"
      },
      "outputs": [],
      "source": [
        "logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "max_eval_samples = (\n",
        "    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n",
        ")\n",
        "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3LSdUdPh7uG"
      },
      "source": [
        "### Predecting the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnXhVq6Yh_oS"
      },
      "outputs": [],
      "source": [
        "id2l = {v: k for k, v in l2id.items()}\n",
        "logger.info(\"*** Predict ***\")\n",
        "#predict_dataset = predict_dataset.remove_columns(\"label\")\n",
        "ids = predict_dataset['id']\n",
        "predict_dataset = predict_dataset.remove_columns(\"id\")\n",
        "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "output_predict_file = os.path.join(training_args.output_dir, f\"subtask_1B.tsv\")\n",
        "if trainer.is_world_process_zero():\n",
        "    with open(output_predict_file, \"w\") as writer:\n",
        "        logger.info(f\"***** Predict results *****\")\n",
        "        writer.write(\"id\\tlabel\\tmodel\\n\")\n",
        "        for index, item in enumerate(predictions):\n",
        "            item = label_list[item]\n",
        "            item = id2l[item]\n",
        "            writer.write(f\"{ids[index]}\\t{item}\\t{model_name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gqqk_24__47"
      },
      "outputs": [],
      "source": [
        "ids[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQgoTTIoiI0X"
      },
      "source": [
        "#### Saving the model into card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1ooJgrViLVj"
      },
      "outputs": [],
      "source": [
        "kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\n",
        "trainer.create_model_card(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2Uq2f0CQXvu"
      },
      "outputs": [],
      "source": [
        "!zip subtask_1B.zip ./distilBERT_m/subtask_1B.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNFwIyDfQ4Sl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}