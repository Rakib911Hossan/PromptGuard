{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZjOpQLseZDq",
    "outputId": "e5c2ac2e-a7dc-40bb-ef6e-b12bae9cef6c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# HATE SPEECH CLASSIFICATION WITH XLM-ROBERTA AND QWEN AUTO-LABELING\n",
    "# Complete Google Colab Notebook\n",
    "# =============================================================================\n",
    "\n",
    "# Cell 1: Install Dependencies\n",
    "# =============================================================================\n",
    "!pip install transformers datasets scikit-learn torch openai accelerate evaluate\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "# Cell 2: Download Dataset\n",
    "# =============================================================================\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_train.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev.tsv\n",
    "!wget https://raw.githubusercontent.com/AridHasan/blp25_task1/refs/heads/main/data/subtask_1A/blp25_hatespeech_subtask_1A_dev_test.tsv\n",
    "\n",
    "# Cell 2: Import Libraries and Setup\n",
    "# =============================================================================\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import evaluate\n",
    "from openai import OpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDNQf3jqerhR",
    "outputId": "c7db01a7-2493-4cf3-c0a7-6e3e3a8c99b0"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define Label Mapping and Optimized Configuration\n",
    "# =============================================================================\n",
    "# Label to ID mapping\n",
    "l2id = {\n",
    "    \"None\": 0,\n",
    "    \"Religious Hate\": 1,\n",
    "    \"Sexism\": 2,\n",
    "    \"Political Hate\": 3,\n",
    "    \"Profane\": 4,\n",
    "    \"Abusive\": 5\n",
    "}\n",
    "\n",
    "id2l = {v: k for k, v in l2id.items()}\n",
    "\n",
    "# ðŸŽ¯ OPTIMIZED CONFIGURATION BASED ON SUCCESSFUL RESULTS\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8                           # Optimized batch size\n",
    "LEARNING_RATE = 10e-6                    # Lower LR for more stable convergence\n",
    "NUM_EPOCHS = 8                           # More epochs with early stopping\n",
    "WARMUP_RATIO = 0.15                      # Longer warmup for stability\n",
    "WEIGHT_DECAY = 0.01                      # Stronger regularization\n",
    "MAX_GRAD_NORM = 0.5                      # Tighter gradient clipping\n",
    "\n",
    "# ðŸŽ¯ OPTIMIZED DATA PARAMETERS\n",
    "max_train_samples = None\n",
    "max_eval_samples = None\n",
    "max_predict_samples = None\n",
    "max_seq_length = 512\n",
    "batch_size = 8\n",
    "\n",
    "print(f\"Label mapping: {l2id}\")\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Optimized configuration loaded with early stopping and regularization\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KngXr_sYTcbH",
    "outputId": "dfc053c8-180b-46ef-d79f-691df8b1fae2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsmyuQkPu9wW"
   },
   "outputs": [],
   "source": [
    "# # Cell 5: Setup Qwen Hugging Face Model for Auto-labeling\n",
    "# # =============================================================================\n",
    "\n",
    "\n",
    "# !pip install huggingface_hub -q\n",
    "\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # Paste your HF token here (get it from https://huggingface.co/settings/tokens)\n",
    "# login(token=\"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\")\n",
    "\n",
    "\n",
    "# !pip install transformers accelerate bitsandbytes -q\n",
    "\n",
    "# import torch\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# # Choose a smaller model if GPU is limited (Colab T4 works better with 0.5B or 1.8B)\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# # Load tokenizer & model with memory-efficient options\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# # Setup pipeline for generation\n",
    "# qwen_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # Label mapping dictionary\n",
    "# l2id = {\n",
    "#     \"None\": 0,\n",
    "#     \"Religious Hate\": 1,\n",
    "#     \"Sexism\": 2,\n",
    "#     \"Political Hate\": 3,\n",
    "#     \"Profane\": 4,\n",
    "#     \"Abusive\": 5\n",
    "# }\n",
    "\n",
    "# def classify_text_with_qwen(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Classify a single text using Qwen from Hugging Face with strict multi-class prompt\n",
    "#     \"\"\"\n",
    "#     prompt = f\"\"\"You are a hate speech classification expert.\n",
    "# Classify the following text into exactly one of these categories:\n",
    "\n",
    "# None, Religious Hate, Sexism, Political Hate, Profane, Abusive\n",
    "\n",
    "# Text: \"{text}\"\n",
    "\n",
    "# Important: Respond with ONLY one category name from the list above.\"\"\"\n",
    "\n",
    "#     try:\n",
    "#         output = qwen_pipeline(\n",
    "#             prompt,\n",
    "#             max_new_tokens=10,\n",
    "#             temperature=0.0,\n",
    "#             do_sample=False\n",
    "#         )[0][\"generated_text\"]\n",
    "\n",
    "#         # Extract the last valid label\n",
    "#         for label in l2id.keys():\n",
    "#             if label in output:\n",
    "#                 return label\n",
    "\n",
    "#         print(f\"âš ï¸ Invalid prediction '{output}' for text: {text[:50]}...\")\n",
    "#         return \"None\"  # fallback\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error classifying text: {e}\")\n",
    "#         return \"None\"\n",
    "\n",
    "# def auto_label_missing_data(df: pd.DataFrame, batch_size: int = 16) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Auto-label missing data using Hugging Face Qwen with efficient batching.\n",
    "#     \"\"\"\n",
    "#     print(\"Starting auto-labeling with Hugging Face Qwen...\")\n",
    "#     df_copy = df.copy()\n",
    "#     missing_mask = df_copy['label'].isna()\n",
    "#     missing_count = missing_mask.sum()\n",
    "\n",
    "#     if missing_count == 0:\n",
    "#         print(\"No missing labels found.\")\n",
    "#         return df_copy\n",
    "\n",
    "#     print(f\"Auto-labeling {missing_count} missing samples...\")\n",
    "\n",
    "#     missing_indices = df_copy[missing_mask].index.tolist()\n",
    "\n",
    "#     for i in range(0, len(missing_indices), batch_size):\n",
    "#         batch_indices = missing_indices[i:i+batch_size]\n",
    "#         texts = df_copy.loc[batch_indices, 'text'].tolist()\n",
    "\n",
    "#         # Build prompts for the batch\n",
    "#         prompts = [\n",
    "#             f\"\"\"You are a hate speech classification expert.\n",
    "# Classify the following text into exactly one of these categories:\n",
    "\n",
    "# None, Religious Hate, Sexism, Political Hate, Profane, Abusive\n",
    "\n",
    "# Text: \"{t}\"\n",
    "\n",
    "# Important: Respond with ONLY one category name from the list above.\"\"\"\n",
    "#             for t in texts\n",
    "#         ]\n",
    "\n",
    "#         try:\n",
    "#             outputs = qwen_pipeline(\n",
    "#                 prompts,\n",
    "#                 max_new_tokens=10,\n",
    "#                 do_sample=False,\n",
    "#                 batch_size=batch_size\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "#             outputs = [[{\"generated_text\": \"\"}] for _ in prompts]\n",
    "\n",
    "#         # âœ… Handle nested list structure\n",
    "#         predicted_labels = []\n",
    "#         for out_group in outputs:\n",
    "#             if isinstance(out_group, list):\n",
    "#                 text_out = out_group[0][\"generated_text\"]\n",
    "#             else:\n",
    "#                 text_out = out_group[\"generated_text\"]\n",
    "\n",
    "#             found = next((label for label in l2id if label in text_out), \"None\")\n",
    "#             predicted_labels.append(found)\n",
    "\n",
    "#         # Assign predictions back to dataframe\n",
    "#         for idx, label in zip(batch_indices, predicted_labels):\n",
    "#             df_copy.loc[idx, 'label'] = label\n",
    "\n",
    "#         print(f\"âœ… Processed batch {i//batch_size + 1}/{(len(missing_indices)-1)//batch_size + 1}\")\n",
    "\n",
    "#     print(\"ðŸŽ‰ Auto-labeling completed!\")\n",
    "#     return df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkTEO5mQe2Mx"
   },
   "outputs": [],
   "source": [
    "# # Cell 6: Load and Process Data\n",
    "# # =============================================================================\n",
    "# def load_and_process_data():\n",
    "#     \"\"\"Load and process all datasets\"\"\"\n",
    "\n",
    "#     # Load datasets\n",
    "#     print(\"Loading datasets...\")\n",
    "#     train_df = pd.read_csv('blp25_hatespeech_subtask_1A_train.tsv', sep='\\t')\n",
    "#     val_df = pd.read_csv('blp25_hatespeech_subtask_1A_dev.tsv', sep='\\t')\n",
    "#     test_df = pd.read_csv('blp25_hatespeech_subtask_1A_dev_test.tsv', sep='\\t')\n",
    "\n",
    "#     print(f\"Original dataset sizes:\")\n",
    "#     print(f\"Train: {len(train_df)} samples\")\n",
    "#     print(f\"Validation: {len(val_df)} samples\")\n",
    "#     print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "#     # Check columns in each dataset\n",
    "#     print(f\"\\nDataset columns:\")\n",
    "#     print(f\"Train columns: {list(train_df.columns)}\")\n",
    "#     print(f\"Validation columns: {list(val_df.columns)}\")\n",
    "#     print(f\"Test columns: {list(test_df.columns)}\")\n",
    "\n",
    "#     # Add label column to test set if it doesn't exist\n",
    "#     if 'label' not in test_df.columns:\n",
    "#         print(\"Test dataset has no 'label' column. Adding empty label column.\")\n",
    "#         test_df['label'] = None\n",
    "\n",
    "#     # Check missing labels\n",
    "#     print(f\"\\nMissing labels:\")\n",
    "#     print(f\"Train: {train_df['label'].isna().sum()}\")\n",
    "#     print(f\"Validation: {val_df['label'].isna().sum()}\")\n",
    "#     print(f\"Test: {test_df['label'].isna().sum()}\")\n",
    "\n",
    "#     # Auto-label missing data\n",
    "#     train_df = auto_label_missing_data(train_df)\n",
    "#     val_df = auto_label_missing_data(val_df)\n",
    "#     test_df = auto_label_missing_data(test_df)\n",
    "\n",
    "#     # Verify no missing labels remain\n",
    "#     print(f\"\\nAfter auto-labeling - Missing labels:\")\n",
    "#     print(f\"Train: {train_df['label'].isna().sum()}\")\n",
    "#     print(f\"Validation: {val_df['label'].isna().sum()}\")\n",
    "#     print(f\"Test: {test_df['label'].isna().sum()}\")\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# # Load the data\n",
    "# train_df, val_df, test_df = load_and_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DpnZ9j0fPmK"
   },
   "outputs": [],
   "source": [
    "# # Cell 7: Exploratory Data Analysis\n",
    "# # =============================================================================\n",
    "# def analyze_data(train_df, val_df, test_df):\n",
    "#     \"\"\"Perform exploratory data analysis\"\"\"\n",
    "\n",
    "#     print(\"=== TRAINING DATA ANALYSIS ===\")\n",
    "#     print(f\"Shape: {train_df.shape}\")\n",
    "#     print(f\"\\nLabel distribution:\")\n",
    "#     label_counts = train_df['label'].value_counts()\n",
    "#     label_percentages = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "#     for label in label_counts.index:\n",
    "#         print(f\"{label}: {label_counts[label]} ({label_percentages[label]:.2f}%)\")\n",
    "\n",
    "#     print(f\"\\nText statistics:\")\n",
    "#     train_df['text_length'] = train_df['text'].str.len()\n",
    "#     print(f\"Mean text length: {train_df['text_length'].mean():.2f}\")\n",
    "#     print(f\"Max text length: {train_df['text_length'].max()}\")\n",
    "#     print(f\"Min text length: {train_df['text_length'].min()}\")\n",
    "\n",
    "#     print(f\"\\nValidation set label distribution:\")\n",
    "#     print(val_df['label'].value_counts())\n",
    "\n",
    "#     return label_counts\n",
    "\n",
    "# label_counts = analyze_data(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdRj7Vamf-6M"
   },
   "outputs": [],
   "source": [
    "# # Cell 8: Convert Labels to Numeric IDs\n",
    "# # =============================================================================\n",
    "# def convert_labels_to_ids(df):\n",
    "#     \"\"\"Convert string labels to numeric IDs\"\"\"\n",
    "#     df_copy = df.copy()\n",
    "#     df_copy['labels'] = df_copy['label'].map(l2id)\n",
    "\n",
    "#     # Check for any unmapped labels\n",
    "#     unmapped = df_copy['labels'].isna().sum()\n",
    "#     if unmapped > 0:\n",
    "#         print(f\"Warning: {unmapped} labels could not be mapped\")\n",
    "#         print(\"Unmapped labels:\", df_copy[df_copy['labels'].isna()]['label'].unique())\n",
    "#         # Fill unmapped with 0 (None)\n",
    "#         df_copy['labels'] = df_copy['labels'].fillna(0)\n",
    "\n",
    "#     return df_copy\n",
    "\n",
    "# import pandas as pd\n",
    "# # Convert labels for all datasets\n",
    "# train_df = convert_labels_to_ids(train_df)\n",
    "# val_df = convert_labels_to_ids(val_df)\n",
    "# test_df = convert_labels_to_ids(test_df)\n",
    "\n",
    "\n",
    "# print(\"Label conversion completed!\")\n",
    "# print(f\"Training set label distribution (numeric):\")\n",
    "# print(train_df['labels'].value_counts().sort_index())\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Example: save converted CSVs to a folder in your Drive\n",
    "# train_df.to_csv('/content/drive/MyDrive/ColabFiles/train_converted.csv', index=False)\n",
    "# val_df.to_csv('/content/drive/MyDrive/ColabFiles/val_converted.csv', index=False)\n",
    "# test_df.to_csv('/content/drive/MyDrive/ColabFiles/test_converted.csv', index=False)\n",
    "\n",
    "\n",
    "# print(\"Files saved to /content folder:\")\n",
    "# print(\" - train_converted.csv\")\n",
    "# print(\" - val_converted.csv\")\n",
    "# print(\" - test_converted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "fb23c31206fa42788f32ec70d656ceb3",
      "1298a90f3e0f4b00a1208228efee3979",
      "e0d6387a86564962aabd5613c2161993",
      "bcef5927d561495583b6af16ed12acbc",
      "e8f836262d004f20aa4180815ceb933f",
      "4d692eccb542460984f8321bac9573b2",
      "a14a03fdaa2b425a814a9ee48e1afe64",
      "84c740128ae04dc0989a44bfca8c6754",
      "eb86010a8e7f4782b282a2a1ff9a2bed",
      "7caa9e32742b4cd1b16460d4e9f06d90",
      "95199f8ffb9045d59f0937a6e135a212",
      "e1ee08e30036461cbc0dd040dfb27de0",
      "ca97c1cda87a4437b634cde4e121750b",
      "6654c78a6d494f178583b43623128051",
      "add853328e914472873a36600b81986d",
      "9bd3a53834654a279b76f55bfa02ee83",
      "beb248bf8c3d4902999fdb89a3811a0d",
      "79b9a78501ed4650a33a700003f19a98",
      "6d723da869de4d76aa693d51c7756b75",
      "7a61f96012824e8c80ee690e513ab64e",
      "a7fb8f2cab7243f0a674c8da6ba9b155",
      "f9b95dde39e04edfb1836a4b72d3f49e",
      "843b5771a7d0424aba70a82af85582a0",
      "cd1e50141ddc429f95eb1cb1806c41fd",
      "6a6d4fc48f3c4b2798069a85e767988c",
      "63fc87deea5c4d998ba77eb0eb0401c9",
      "076261586ced450397179f6bf1aadfb4",
      "0da9e57d01004097ac5b31fd6cb332ec",
      "91dbbccf74f346f1a84b54825cde3d03",
      "4c3f8ed9e18e4b6fb9f3e50b369b00f1",
      "f1827cb8b1604c1ea0693cbe3008b52f",
      "4c888f4439af456c91f0e859110b4bd2",
      "eca22c006c6a42c697476076d28a388d"
     ]
    },
    "id": "FpqROfNFf_f_",
    "outputId": "a00b8eee-ff74-4db3-d2e9-63876d520073"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Setup Tokenization\n",
    "# =============================================================================\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure pad token is defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # recommended for decoder-only models\n",
    "\n",
    "# Updated tokenize function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text data with padding for batching\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # pad to MAX_LENGTH\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load your saved CSVs from Drive\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/train_converted.csv')\n",
    "val_df   = pd.read_csv('/content/drive/MyDrive/ColabFiles/val_converted.csv')\n",
    "test_df  = pd.read_csv('/content/drive/MyDrive/ColabFiles/test_converted.csv')\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "print(\"Converting to HuggingFace datasets...\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "val_dataset   = Dataset.from_pandas(val_df[['text', 'labels']])\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text', 'labels']])\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for Trainer\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Dataset sizes after tokenization:\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Validation: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdnNAzF5fkib",
    "outputId": "8136dbf3-5cef-4626-d854-e1597aa0b97e"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Exploratory Data Analysis\n",
    "# =============================================================================\n",
    "def analyze_data(train_df, val_df, test_df):\n",
    "    \"\"\"Perform exploratory data analysis\"\"\"\n",
    "\n",
    "    print(\"=== TRAINING DATA ANALYSIS ===\")\n",
    "    print(f\"Shape: {train_df.shape}\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    label_counts = train_df['label'].value_counts()\n",
    "    label_percentages = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "    for label in label_counts.index:\n",
    "        print(f\"{label}: {label_counts[label]} ({label_percentages[label]:.2f}%)\")\n",
    "\n",
    "    print(f\"\\nText statistics:\")\n",
    "    train_df['text_length'] = train_df['text'].str.len()\n",
    "    print(f\"Mean text length: {train_df['text_length'].mean():.2f}\")\n",
    "    print(f\"Max text length: {train_df['text_length'].max()}\")\n",
    "    print(f\"Min text length: {train_df['text_length'].min()}\")\n",
    "\n",
    "    print(f\"\\nValidation set label distribution:\")\n",
    "    print(val_df['label'].value_counts())\n",
    "\n",
    "    #     # Check missing labels\n",
    "    print(f\"\\nMissing labels:\")\n",
    "    print(f\"Train: {train_df['label'].isna().sum()}\")\n",
    "    print(f\"Validation: {val_df['label'].isna().sum()}\")\n",
    "    print(f\"Test: {test_df['label'].isna().sum()}\")\n",
    "\n",
    "\n",
    "    return label_counts\n",
    "\n",
    "label_counts = analyze_data(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUYewGz9gC-e",
    "outputId": "66154fb5-7d7f-4b84-a838-cbe0623892cf"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Compute Class Weights for Balanced Training\n",
    "# =============================================================================\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights for balanced training\"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "\n",
    "    # Create weight dict\n",
    "    weight_dict = {int(label): weight for label, weight in zip(unique_labels, class_weights)}\n",
    "\n",
    "    print(\"Class weights:\")\n",
    "    for label_id, weight in weight_dict.items():\n",
    "        label_name = id2l[label_id]\n",
    "        print(f\"  {label_name} (ID: {label_id}): {weight:.3f}\")\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(train_df['labels'].values)\n",
    "\n",
    "# Cell 11: Custom Trainer with Weighted Loss\n",
    "# =============================================================================\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with weighted CrossEntropyLoss\"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"Compute weighted loss\"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Move class weights to same device as logits\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(logits.device)\n",
    "            loss_fct = CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljH0DqgDgIwd",
    "outputId": "58a11447-19bb-4356-8a5a-61ad51ce2e8a"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Compute Class Weights for Balanced Training\n",
    "# =============================================================================\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights for balanced training\"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "\n",
    "    # Create weight dict\n",
    "    weight_dict = {int(label): weight for label, weight in zip(unique_labels, class_weights)}\n",
    "\n",
    "    print(\"Class weights:\")\n",
    "    for label_id, weight in weight_dict.items():\n",
    "        label_name = id2l[label_id]\n",
    "        print(f\"  {label_name} (ID: {label_id}): {weight:.3f}\")\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(train_df['labels'].values)\n",
    "\n",
    "# Cell 11: Custom Trainer with Weighted Loss\n",
    "# =============================================================================\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with weighted CrossEntropyLoss\"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Compute weighted loss\"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Move class weights to same device as logits\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(logits.device)\n",
    "            loss_fct = CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6Dnqd2Wgyrp"
   },
   "outputs": [],
   "source": [
    "# Cell 12: Define Evaluation Metrics\n",
    "# =============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "    }\n",
    "\n",
    "    # Add per-class metrics\n",
    "    for i, label_name in id2l.items():\n",
    "        if i < len(f1_per_class):\n",
    "            metrics[f'f1_{label_name.replace(\" \", \"_\")}'] = f1_per_class[i]\n",
    "            metrics[f'precision_{label_name.replace(\" \", \"_\")}'] = precision_per_class[i]\n",
    "            metrics[f'recall_{label_name.replace(\" \", \"_\")}'] = recall_per_class[i]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233,
     "referenced_widgets": [
      "b0e6bd6c579940178aed62bcb8f56f7a",
      "de37477cfe7b44be9ae9e4cf407abfb1",
      "452033d5a6b34be88ac1ad6778669b9f",
      "b3fa26344d3e4705a71199a07a92c24c",
      "cc246759c5a747edae1e79c636723643",
      "6dfb8234c9004ff4aa7656937c185df0",
      "317f27dbd0714d0db58e5ff0ebf05b1a",
      "9c19c6f0e67e488fadf431acadec086f",
      "a4523c7794e542e6b36cdefc239d6910",
      "04949eb231994e20a7a827ebb03f1578",
      "e94938d104d54edd97d9a4fc549dd5aa"
     ]
    },
    "id": "oq09NuJLgPFM",
    "outputId": "cca1e8fc-4235-4048-d127-de8a1a0c901c"
   },
   "outputs": [],
   "source": [
    "# Cell 13: Load Model and Setup Optimized Training Configuration\n",
    "# =============================================================================\n",
    "print(\"Loading model...\")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(l2id),\n",
    "    id2label=id2l,\n",
    "    label2id=l2id,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=6,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32  # use FP32 instead\n",
    ")\n",
    "\n",
    "# Tell the model what pad token ID to use\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Model loaded with {len(l2id)} classes\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ðŸŽ¯ OPTIMIZED TRAINING ARGUMENTS BASED ON YOUR SUCCESSFUL CONFIGURATION\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm_roberta_recovery/\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # ðŸ”§ ADJUSTED LEARNING SCHEDULE - Lower LR for stable convergence\n",
    "    learning_rate=2e-5,           # 10e-6 for stability\n",
    "    num_train_epochs=4,           # 8 epochs with early stopping\n",
    "    warmup_ratio=WARMUP_RATIO,             # 0.15 for longer warmup\n",
    "    lr_scheduler_type=\"cosine\",            # Smoother LR decay\n",
    "\n",
    "    # ðŸ›¡ï¸ STRONGER REGULARIZATION - Combat overfitting\n",
    "    weight_decay=WEIGHT_DECAY,             # 0.01 regularization\n",
    "    max_grad_norm=MAX_GRAD_NORM,           # 0.5 gradient clipping\n",
    "    dataloader_drop_last=True,             # Consistent batch sizes\n",
    "\n",
    "    # âœ… OPTIMIZED BATCH CONFIGURATION\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    # ðŸŽ¯ EARLY STOPPING CONFIGURATION\n",
    "    eval_strategy=\"epoch\",           # Eval every epoch\n",
    "    save_strategy=\"epoch\",                 # Save every epoch\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\", # Use macro F1 as primary metric\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # ðŸ“Š COMPREHENSIVE MONITORING\n",
    "    logging_steps=250,                     # More frequent logging\n",
    "    save_total_limit=3,                    # Keep only best 3 checkpoints\n",
    "\n",
    "    # ðŸ”§ SYSTEM OPTIMIZATIONS\n",
    "    report_to=None,                        # Disable wandb\n",
    "    dataloader_num_workers=2,\n",
    "    fp16=False,                            # Mixed precision for efficiency\n",
    "    group_by_length=True,                 # Batch similar lengths together\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# ðŸ›‘ MANDATORY EARLY STOPPING - Prevent overfitting\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,            # Stop after 2 epochs without improvement\n",
    "    early_stopping_threshold=0.001        # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "print(\"Optimized training arguments configured with early stopping\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Early stopping patience: 2 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PifZouKFHiAn",
    "outputId": "3052911d-2a48-4f0a-ab88-3879e61440fe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "M86tSVOHgSu6",
    "outputId": "021de48b-366d-4ffa-b9ea-f2fbc8ffad9f"
   },
   "outputs": [],
   "source": [
    "# Cell 14: Initialize Trainer and Start Training\n",
    "# =============================================================================\n",
    "# Initialize trainer with weighted loss\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Validating on {len(val_dataset)} samples\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "AzyaFicqgh9p",
    "outputId": "3f82a3a7-2b70-4ffd-be15-052d762a277f"
   },
   "outputs": [],
   "source": [
    "# Cell 15: Evaluation on Test Set\n",
    "# =============================================================================\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "test_labels = test_predictions.label_ids\n",
    "\n",
    "# Compute final metrics\n",
    "final_metrics = compute_metrics((test_predictions.predictions, test_labels))\n",
    "\n",
    "print(\"=== FINAL TEST RESULTS ===\")\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Macro F1: {final_metrics['f1_macro']:.4f}\")\n",
    "print(f\"Macro Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "print(f\"Macro Recall: {final_metrics['recall_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_O1NmBFohDFB",
    "outputId": "8eff7716-de90-4287-90b2-c67bcf77aa11"
   },
   "outputs": [],
   "source": [
    "# Cell 16: Detailed Classification Report\n",
    "# =============================================================================\n",
    "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "\n",
    "# Generate classification report\n",
    "target_names = [id2l[i] for i in range(len(l2id))]\n",
    "report = classification_report(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    target_names=target_names,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Print per-class results\n",
    "print(\"\\n=== PER-CLASS RESULTS ===\")\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "for i, label_name in enumerate(target_names):\n",
    "    if i < len(f1_per_class):\n",
    "        print(f\"{label_name}:\")\n",
    "        print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
    "        print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
    "        print(f\"  F1-Score: {f1_per_class[i]:.4f}\")\n",
    "        print(f\"  Support: {support_per_class[i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "fMWmctrNZarL",
    "outputId": "ca4c0e34-810d-4b40-defa-42744ee39369"
   },
   "outputs": [],
   "source": [
    " git config --global user.email \"rakib911hossan@gmail.com\"\n",
    "  git config --global user.name \"Rakib911Hossan\"\n",
    "\n",
    "!git clone https://github.com/Rakib911Hossan/hate_speech_detection.git\n",
    "%cd hate_speech_detection\n",
    "# move your notebook here\n",
    "!git add subtask_1a_xlm_roberta_qwen_label2.ipynb\n",
    "!git commit -m \"Update notebook\"\n",
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlKpfHOIiypi"
   },
   "outputs": [],
   "source": [
    "# Cell 17: Save Optimized Model and Results\n",
    "# =============================================================================\n",
    "print(\"ðŸ’¾ Saving optimized model and tokenizer...\")\n",
    "\n",
    "# Save the fine-tuned model with optimized configuration\n",
    "model.save_pretrained(\"./xlm-roberta-hate-speech-optimized\")\n",
    "tokenizer.save_pretrained(\"./xlm-roberta-hate-speech-optimized\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'text': test_df['text'].values,\n",
    "    'true_label': [id2l[label] for label in test_labels],\n",
    "    'predicted_label': [id2l[pred] for pred in test_preds],\n",
    "    'correct': test_labels == test_preds\n",
    "})\n",
    "\n",
    "results_df.to_csv('optimized_test_results.csv', index=False)\n",
    "\n",
    "# Save training configuration for reference\n",
    "config_info = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'max_epochs': NUM_EPOCHS,\n",
    "    'warmup_ratio': WARMUP_RATIO,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'early_stopping_patience': 2,\n",
    "    'scheduler': 'cosine',\n",
    "    'fp16': True,\n",
    "    'final_test_accuracy': (test_labels == test_preds).mean(),\n",
    "    'final_macro_f1': final_metrics['f1_macro']\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('training_config.json', 'w') as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(\"âœ… Model and results saved!\")\n",
    "print(f\"ðŸŽ¯ Test accuracy: {(test_labels == test_preds).mean():.4f}\")\n",
    "print(f\"ðŸ“ˆ Macro F1: {final_metrics['f1_macro']:.4f}\")\n",
    "print(f\"ðŸ“ Model saved to: ./xlm-roberta-hate-speech-optimized\")\n",
    "print(f\"ðŸ“Š Results saved to: optimized_test_results.csv\")\n",
    "print(f\"âš™ï¸ Config saved to: training_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IB0fdupUvbva"
   },
   "outputs": [],
   "source": [
    "# Cell 18: Example Predictions\n",
    "# =============================================================================\n",
    "def predict_text(text, model, tokenizer, device='cpu'):\n",
    "    \"\"\"Predict label for a single text\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_id = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_id].item()\n",
    "\n",
    "    return id2l[predicted_id], confidence\n",
    "\n",
    "# Test with some examples\n",
    "print(\"\\n=== EXAMPLE PREDICTIONS ===\")\n",
    "example_texts = [\n",
    "    \"This is a normal text with no hate speech\",\n",
    "    \"I hate all politicians they are corrupt\",\n",
    "    \"Religious people are stupid and should be eliminated\",\n",
    "    \"Women belong in the kitchen not in workplace\"\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for text in example_texts:\n",
    "    pred_label, confidence = predict_text(text, model, tokenizer, device)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Prediction: {pred_label} (confidence: {confidence:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\"=== TRAINING COMPLETE ===\")\n",
    "print(\"Your hate speech classifier is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgLzUzxKvck9"
   },
   "outputs": [],
   "source": [
    "# Cell 19: Optimized Model Summary and Final Statistics\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ OPTIMIZED TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ¤– Model: {MODEL_NAME}\")\n",
    "print(f\"ðŸ“Š Training samples: {len(train_dataset)}\")\n",
    "print(f\"ðŸ“Š Validation samples: {len(val_dataset)}\")\n",
    "print(f\"ðŸ“Š Test samples: {len(test_dataset)}\")\n",
    "print(f\"ðŸ·ï¸  Number of classes: {len(l2id)}\")\n",
    "print(f\"ðŸ“ Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ OPTIMIZED HYPERPARAMETERS:\")\n",
    "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  â€¢ Max epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  â€¢ Warmup ratio: {WARMUP_RATIO}\")\n",
    "print(f\"  â€¢ Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  â€¢ Gradient clipping: {MAX_GRAD_NORM}\")\n",
    "print(f\"  â€¢ Scheduler: cosine\")\n",
    "print(f\"  â€¢ Early stopping: patience=2\")\n",
    "print(f\"  â€¢ Mixed precision: FP16\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ FINAL PERFORMANCE:\")\n",
    "print(f\"  â€¢ Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"  â€¢ Macro F1-Score: {final_metrics['f1_macro']:.4f}\")\n",
    "print(f\"  â€¢ Macro Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "print(f\"  â€¢ Macro Recall: {final_metrics['recall_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ FILES GENERATED:\")\n",
    "print(f\"  â€¢ ./xlm-roberta-hate-speech-optimized/ (optimized model)\")\n",
    "print(f\"  â€¢ optimized_test_results.csv (detailed predictions)\")\n",
    "print(f\"  â€¢ training_config.json (hyperparameter configuration)\")\n",
    "\n",
    "print(f\"\\nðŸ”„ TO LOAD THE OPTIMIZED MODEL LATER:\")\n",
    "print(\"```python\")\n",
    "print(\"from transformers import AutoModelForSequenceClassification, AutoTokenizer\")\n",
    "print(\"model = AutoModelForSequenceClassification.from_pretrained('./xlm-roberta-hate-speech-optimized')\")\n",
    "print(\"tokenizer = AutoTokenizer.from_pretrained('./xlm-roberta-hate-speech-optimized')\")\n",
    "print(\"```\")\n",
    "\n",
    "print(f\"\\nðŸš€ OPTIMIZATIONS APPLIED:\")\n",
    "print(\"  âœ… Lower learning rate for stable convergence\")\n",
    "print(\"  âœ… Early stopping to prevent overfitting\")\n",
    "print(\"  âœ… Cosine learning rate scheduling\")\n",
    "print(\"  âœ… Stronger regularization (weight decay)\")\n",
    "print(\"  âœ… Gradient clipping for stability\")\n",
    "print(\"  âœ… Mixed precision training\")\n",
    "print(\"  âœ… Weighted loss for class balance\")\n",
    "print(\"  âœ… Longer warmup period\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽŠ Your optimized hate speech classifier is ready!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
